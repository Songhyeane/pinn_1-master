{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN_NeuralNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "            output_dim=1,\n",
    "            num_hidden_layers=8,\n",
    "            num_neurons_per_layer=20,\n",
    "            activation='tanh',\n",
    "            kernel_initializer='glorot_normal',\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        self.scale = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        Z = self.scale(X)\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        return self.out(Z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "class PINNSolver():\n",
    "    def __init__(self, model, X_r):\n",
    "        self.model = model\n",
    "\n",
    "        # Store collocation points\n",
    "        self.t = X_r[:,0:1]\n",
    "        self.x = X_r[:,1:2]\n",
    "\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def get_r(self):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variables representing t and x during this GradientTape\n",
    "            tape.watch(self.t)\n",
    "            tape.watch(self.x)\n",
    "\n",
    "            # Compute current values u(t,x)\n",
    "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
    "\n",
    "            u_x = tape.gradient(u, self.x)\n",
    "\n",
    "        u_t = tape.gradient(u, self.t)\n",
    "        u_xx = tape.gradient(u_x, self.x)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
    "\n",
    "    def loss_fn(self, X, u):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_r()\n",
    "        phi_r = tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = phi_r\n",
    "\n",
    "        # Add phi_0 and phi_b to the loss\n",
    "        for i in range(len(X)):\n",
    "            u_pred = self.model(X[i])\n",
    "            loss += tf.reduce_mean(tf.square(u[i] - u_pred))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_grad(self, X, u):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with\n",
    "            # respect to trainable variables\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            loss = self.loss_fn(X, u)\n",
    "\n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        del tape\n",
    "\n",
    "        return loss, g\n",
    "\n",
    "    def fun_r(self, t, x, u, u_t, u_x, u_xx):\n",
    "        \"\"\"Residual of the PDE\"\"\"\n",
    "        return u_t + u * u_x - viscosity * u_xx\n",
    "\n",
    "    def solve_with_TFoptimizer(self, optimizer, X, u, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "\n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            loss, grad_theta = self.get_grad(X, u)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        for i in range(N):\n",
    "\n",
    "            loss = train_step()\n",
    "\n",
    "            self.current_loss = loss.numpy()\n",
    "            self.callback()\n",
    "\n",
    "    def solve_with_ScipyOptimizer(self, X, u, method='L-BFGS-B', **kwargs):\n",
    "        \"\"\"This method provides an interface to solve the learning problem\n",
    "        using a routine from scipy.optimize.minimize.\n",
    "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
    "        supported in Tensorflow 2.xx.)\n",
    "        Type conversion is necessary since scipy-routines are written in Fortran\n",
    "        which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
    "\n",
    "        def get_weight_tensor():\n",
    "            \"\"\"Function to return current variables of the model\n",
    "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
    "\n",
    "            weight_list = []\n",
    "            shape_list = []\n",
    "\n",
    "            # Loop over all variables, i.e. weight matrices, bias vectors and unknown parameters\n",
    "            for v in self.model.variables:\n",
    "                shape_list.append(v.shape)\n",
    "                weight_list.extend(v.numpy().flatten())\n",
    "\n",
    "            weight_list = tf.convert_to_tensor(weight_list)\n",
    "            return weight_list, shape_list\n",
    "\n",
    "        x0, shape_list = get_weight_tensor()\n",
    "\n",
    "        def set_weight_tensor(weight_list):\n",
    "            \"\"\"Function which sets list of weights\n",
    "            to variables in the model.\"\"\"\n",
    "            idx = 0\n",
    "            for v in self.model.variables:\n",
    "                vs = v.shape\n",
    "\n",
    "                # Weight matrices\n",
    "                if len(vs) == 2:\n",
    "                    sw = vs[0]*vs[1]\n",
    "                    new_val = tf.reshape(weight_list[idx:idx+sw],(vs[0],vs[1]))\n",
    "                    idx += sw\n",
    "\n",
    "                # Bias vectors\n",
    "                elif len(vs) == 1:\n",
    "                    new_val = weight_list[idx:idx+vs[0]]\n",
    "                    idx += vs[0]\n",
    "\n",
    "                # Variables (in case of parameter identification setting)\n",
    "                elif len(vs) == 0:\n",
    "                    new_val = weight_list[idx]\n",
    "                    idx += 1\n",
    "\n",
    "                # Assign variables (Casting necessary since scipy requires float64 type)\n",
    "                v.assign(tf.cast(new_val, DTYPE))\n",
    "\n",
    "        def get_loss_and_grad(w):\n",
    "            \"\"\"Function that provides current loss and gradient\n",
    "            w.r.t the trainable variables as vector. This is mandatory\n",
    "            for the LBFGS minimizer from scipy.\"\"\"\n",
    "\n",
    "            # Update weights in model\n",
    "            set_weight_tensor(w)\n",
    "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
    "            loss, grad = self.get_grad(X, u)\n",
    "\n",
    "            # Store current loss for callback function\n",
    "            loss = loss.numpy().astype(np.float64)\n",
    "            self.current_loss = loss\n",
    "\n",
    "            # Flatten gradient\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.extend(g.numpy().flatten())\n",
    "\n",
    "            # Gradient list to array\n",
    "            grad_flat = np.array(grad_flat,dtype=np.float64)\n",
    "\n",
    "            # Return value and gradient of \\phi as tuple\n",
    "            return loss, grad_flat\n",
    "\n",
    "\n",
    "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
    "                                       x0=x0,\n",
    "                                       jac=True,\n",
    "                                       method=method,\n",
    "                                       callback=self.callback,\n",
    "                                       **kwargs)\n",
    "\n",
    "    def callback(self, xr=None):\n",
    "        if self.iter % 50 == 0:\n",
    "            print('It {:05d}: loss = {:10.8e}'.format(self.iter,self.current_loss))\n",
    "        self.hist.append(self.current_loss)\n",
    "        self.iter+=1\n",
    "\n",
    "\n",
    "    def plot_solution(self, **kwargs):\n",
    "        N = 600\n",
    "        tspace = np.linspace(self.model.lb[0], self.model.ub[0], N+1)\n",
    "        xspace = np.linspace(self.model.lb[1], self.model.ub[1], N+1)\n",
    "        T, X = np.meshgrid(tspace, xspace)\n",
    "        Xgrid = np.vstack([T.flatten(),X.flatten()]).T\n",
    "        upred = self.model(tf.cast(Xgrid,DTYPE))\n",
    "        U = upred.numpy().reshape(N+1,N+1)\n",
    "        fig = plt.figure(figsize=(9,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(T, X, U, cmap='viridis', **kwargs)\n",
    "        ax.set_xlabel('$t$')\n",
    "        ax.set_ylabel('$x$')\n",
    "        ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
    "        ax.view_init(35,35)\n",
    "        return ax\n",
    "\n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7,5))\n",
    "            ax = fig.add_subplot(111)\n",
    "        ax.semilogy(range(len(self.hist)), self.hist,'k-')\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
    "        return ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PINN_NeuralNet(lb, ub)\n",
    "model.build(input_shape=(None,2))\n",
    "\n",
    "# Initilize PINN solver\n",
    "solver = PINNSolver(model, X_r)\n",
    "\n",
    "# Decide which optimizer should be used\n",
    "#mode = 'TFoptimizer'\n",
    "mode = 'ScipyOptimizer'\n",
    "\n",
    "# Start timer\n",
    "t0 = time()\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    # Choose optimizer\n",
    "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    solver.solve_with_TFoptimizer(optim, X_data, u_data, N=4001)\n",
    "\n",
    "elif mode == 'ScipyOptimizer':\n",
    "    solver.solve_with_ScipyOptimizer(X_data, u_data,\n",
    "                            method='L-BFGS-B',\n",
    "                            options={'maxiter': 50000,\n",
    "                                     'maxfun': 50000,\n",
    "                                     'maxcor': 50,\n",
    "                                     'maxls': 50,\n",
    "                                     'ftol': 1.0*np.finfo(float).eps})\n",
    "\n",
    "# Print computation time\n",
    "print('\\nComputation time: {} seconds'.format(time()-t0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solver.plot_solution();\n",
    "solver.plot_loss_history();"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
